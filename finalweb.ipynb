{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('parser', <spacy.pipeline.pipes.DependencyParser at 0x29ab40bf528>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import preprocessor as p\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk import stem\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nlp = spacy.load('en_core_web_md', disable=['ner'])\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load json into dataframe\n",
    "with open('sampledata.json') as f:\n",
    "    data=[]\n",
    "    for line in f:\n",
    "        if line!=\"\\n\":\n",
    "            data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(data)\n",
    "df = d[['id','created_at','text','lang']]\n",
    "#df['hashtags']= np.array([i['entities']['hashtags']for i in data])\n",
    "#df['hashtags']= [i['entities']['hashtags']for i in data]\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1240987925296205824</td>\n",
       "      <td>Fri Mar 20 13:05:52 +0000 2020</td>\n",
       "      <td>RT @loofranchot: Deploy the army now I dont ca...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1240987925736607745</td>\n",
       "      <td>Fri Mar 20 13:05:52 +0000 2020</td>\n",
       "      <td>Huxley Sleep Mask ของ LazMall Official จะลดเหล...</td>\n",
       "      <td>th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1240987926399463424</td>\n",
       "      <td>Fri Mar 20 13:05:52 +0000 2020</td>\n",
       "      <td>RT @Danny_Walterr: So an Actor who attended th...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1240987926386933762</td>\n",
       "      <td>Fri Mar 20 13:05:52 +0000 2020</td>\n",
       "      <td>RT @JimbauxsJournal: @JM_Sievert @inLaurasWord...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1240987928819412992</td>\n",
       "      <td>Fri Mar 20 13:05:53 +0000 2020</td>\n",
       "      <td>@alinmango Duduk kt ward yg sama..biar tahu ra...</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>1240988971095560192</td>\n",
       "      <td>Fri Mar 20 13:10:01 +0000 2020</td>\n",
       "      <td>please jangan sakitin baby joyi-ku \\ndia g jah...</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>1240988972282793986</td>\n",
       "      <td>Fri Mar 20 13:10:02 +0000 2020</td>\n",
       "      <td>#Babushka #kids</td>\n",
       "      <td>und</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>1240988972710387714</td>\n",
       "      <td>Fri Mar 20 13:10:02 +0000 2020</td>\n",
       "      <td>RT @DrAmalinaBakri: Siapa yg menjual mask dan ...</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>1240988972895162368</td>\n",
       "      <td>Fri Mar 20 13:10:02 +0000 2020</td>\n",
       "      <td>RT @PhilippeMurer: Les maires plus intelligent...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>1240988972916117505</td>\n",
       "      <td>Fri Mar 20 13:10:02 +0000 2020</td>\n",
       "      <td>Tips for virtual dating in the age of coronavi...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1741 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                      created_at  \\\n",
       "0     1240987925296205824  Fri Mar 20 13:05:52 +0000 2020   \n",
       "1     1240987925736607745  Fri Mar 20 13:05:52 +0000 2020   \n",
       "2     1240987926399463424  Fri Mar 20 13:05:52 +0000 2020   \n",
       "3     1240987926386933762  Fri Mar 20 13:05:52 +0000 2020   \n",
       "4     1240987928819412992  Fri Mar 20 13:05:53 +0000 2020   \n",
       "...                   ...                             ...   \n",
       "1736  1240988971095560192  Fri Mar 20 13:10:01 +0000 2020   \n",
       "1737  1240988972282793986  Fri Mar 20 13:10:02 +0000 2020   \n",
       "1738  1240988972710387714  Fri Mar 20 13:10:02 +0000 2020   \n",
       "1739  1240988972895162368  Fri Mar 20 13:10:02 +0000 2020   \n",
       "1740  1240988972916117505  Fri Mar 20 13:10:02 +0000 2020   \n",
       "\n",
       "                                                   text lang  \n",
       "0     RT @loofranchot: Deploy the army now I dont ca...   en  \n",
       "1     Huxley Sleep Mask ของ LazMall Official จะลดเหล...   th  \n",
       "2     RT @Danny_Walterr: So an Actor who attended th...   en  \n",
       "3     RT @JimbauxsJournal: @JM_Sievert @inLaurasWord...   en  \n",
       "4     @alinmango Duduk kt ward yg sama..biar tahu ra...   in  \n",
       "...                                                 ...  ...  \n",
       "1736  please jangan sakitin baby joyi-ku \\ndia g jah...   in  \n",
       "1737                                    #Babushka #kids  und  \n",
       "1738  RT @DrAmalinaBakri: Siapa yg menjual mask dan ...   in  \n",
       "1739  RT @PhilippeMurer: Les maires plus intelligent...   fr  \n",
       "1740  Tips for virtual dating in the age of coronavi...   en  \n",
       "\n",
       "[1741 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1240987925296205824</td>\n",
       "      <td>Fri Mar 20 13:05:52 +0000 2020</td>\n",
       "      <td>RT @loofranchot: Deploy the army now I dont ca...</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1240987925736607745</td>\n",
       "      <td>Fri Mar 20 13:05:52 +0000 2020</td>\n",
       "      <td>Huxley Sleep Mask ของ LazMall Official จะลดเหล...</td>\n",
       "      <td>th</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1240987926399463424</td>\n",
       "      <td>Fri Mar 20 13:05:52 +0000 2020</td>\n",
       "      <td>RT @Danny_Walterr: So an Actor who attended th...</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1240987926386933762</td>\n",
       "      <td>Fri Mar 20 13:05:52 +0000 2020</td>\n",
       "      <td>RT @JimbauxsJournal: @JM_Sievert @inLaurasWord...</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1240987928819412992</td>\n",
       "      <td>Fri Mar 20 13:05:53 +0000 2020</td>\n",
       "      <td>@alinmango Duduk kt ward yg sama..biar tahu ra...</td>\n",
       "      <td>in</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>1240988971095560192</td>\n",
       "      <td>Fri Mar 20 13:10:01 +0000 2020</td>\n",
       "      <td>please jangan sakitin baby joyi-ku \\ndia g jah...</td>\n",
       "      <td>in</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>1240988972282793986</td>\n",
       "      <td>Fri Mar 20 13:10:02 +0000 2020</td>\n",
       "      <td>#Babushka #kids</td>\n",
       "      <td>und</td>\n",
       "      <td>[{'text': 'Babushka', 'indices': [0, 9]}, {'te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>1240988972710387714</td>\n",
       "      <td>Fri Mar 20 13:10:02 +0000 2020</td>\n",
       "      <td>RT @DrAmalinaBakri: Siapa yg menjual mask dan ...</td>\n",
       "      <td>in</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>1240988972895162368</td>\n",
       "      <td>Fri Mar 20 13:10:02 +0000 2020</td>\n",
       "      <td>RT @PhilippeMurer: Les maires plus intelligent...</td>\n",
       "      <td>fr</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>1240988972916117505</td>\n",
       "      <td>Fri Mar 20 13:10:02 +0000 2020</td>\n",
       "      <td>Tips for virtual dating in the age of coronavi...</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1741 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                      created_at  \\\n",
       "0     1240987925296205824  Fri Mar 20 13:05:52 +0000 2020   \n",
       "1     1240987925736607745  Fri Mar 20 13:05:52 +0000 2020   \n",
       "2     1240987926399463424  Fri Mar 20 13:05:52 +0000 2020   \n",
       "3     1240987926386933762  Fri Mar 20 13:05:52 +0000 2020   \n",
       "4     1240987928819412992  Fri Mar 20 13:05:53 +0000 2020   \n",
       "...                   ...                             ...   \n",
       "1736  1240988971095560192  Fri Mar 20 13:10:01 +0000 2020   \n",
       "1737  1240988972282793986  Fri Mar 20 13:10:02 +0000 2020   \n",
       "1738  1240988972710387714  Fri Mar 20 13:10:02 +0000 2020   \n",
       "1739  1240988972895162368  Fri Mar 20 13:10:02 +0000 2020   \n",
       "1740  1240988972916117505  Fri Mar 20 13:10:02 +0000 2020   \n",
       "\n",
       "                                                   text lang  \\\n",
       "0     RT @loofranchot: Deploy the army now I dont ca...   en   \n",
       "1     Huxley Sleep Mask ของ LazMall Official จะลดเหล...   th   \n",
       "2     RT @Danny_Walterr: So an Actor who attended th...   en   \n",
       "3     RT @JimbauxsJournal: @JM_Sievert @inLaurasWord...   en   \n",
       "4     @alinmango Duduk kt ward yg sama..biar tahu ra...   in   \n",
       "...                                                 ...  ...   \n",
       "1736  please jangan sakitin baby joyi-ku \\ndia g jah...   in   \n",
       "1737                                    #Babushka #kids  und   \n",
       "1738  RT @DrAmalinaBakri: Siapa yg menjual mask dan ...   in   \n",
       "1739  RT @PhilippeMurer: Les maires plus intelligent...   fr   \n",
       "1740  Tips for virtual dating in the age of coronavi...   en   \n",
       "\n",
       "                                               hashtags  \n",
       "0                                                    []  \n",
       "1                                                    []  \n",
       "2                                                    []  \n",
       "3                                                    []  \n",
       "4                                                    []  \n",
       "...                                                 ...  \n",
       "1736                                                 []  \n",
       "1737  [{'text': 'Babushka', 'indices': [0, 9]}, {'te...  \n",
       "1738                                                 []  \n",
       "1739                                                 []  \n",
       "1740                                                 []  \n",
       "\n",
       "[1741 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in case entities is not existing\n",
    "hashtagspre=[]\n",
    "for i in data:\n",
    "    if 'entities'in i.keys():\n",
    "        hashtagspre.append(i['entities']['hashtags'])\n",
    "    else:\n",
    "        hashtagspre.append('')\n",
    "df['hashtags'] = hashtagspre\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text in hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract hashtags\n",
    "hashtags_list = list(df.hashtags)\n",
    "all_list=[]\n",
    "for item in hashtags_list:\n",
    "    text_list = []\n",
    "    for i in item:\n",
    "        text_list.append(i['text'].lower())\n",
    "    all_list.append(text_list)\n",
    "df['textlist'] = all_list\n",
    "\n",
    "## extract english text \n",
    "df = df[df['lang']=='en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select emojis\n",
    "def extract_emojis(str):\n",
    "    return ''.join(c for c in str if c in emoji.UNICODE_EMOJI)\n",
    "emoji_list=[]\n",
    "for i in df['text']:\n",
    "    emoji_list.append(extract_emojis(str(i)))\n",
    "df['emoji'] = emoji_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor clean data\n",
    "clean_text=[]\n",
    "for i in df['text']:\n",
    "    clean_text.append(p.clean(str(i)))\n",
    "df['clean_text'] = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data function\n",
    "def clean_tweets(tweet):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    #after tweepy preprocessing the colon symbol left remain after      #removing mentions\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "#    #filter using NLTK library append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_tweet = []\n",
    "    #looping through conditions\n",
    "    for w in word_tokens:\n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in string.punctuation:\n",
    "            filtered_tweet.append(w)\n",
    "#    return filtered_tweet\n",
    "    return ' '.join(filtered_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling clean data \n",
    "filter_text=[]\n",
    "for i in df['clean_text']:\n",
    "    filter_text.append(clean_tweets(str(i)))\n",
    "df['filter_text'] = filter_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob\n",
    "#from textblob import TextBlob\n",
    "#blob = TextBlob(filtered_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Tokenize\n",
    "def spacy_tokenize(string):\n",
    "    tokens = list()\n",
    "    doc = nlp(string)\n",
    "    for token in doc:\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Normalize\n",
    "def normalize(tokens):\n",
    "    normalized_tokens = list()\n",
    "    for token in tokens:\n",
    "        normalized = token.text.lower().strip()\n",
    "        if ((token.is_alpha or token.is_digit)):\n",
    "            normalized_tokens.append(normalized)\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Tokenize and normalize\n",
    "def tokenize_normalize(string):\n",
    "    return normalize(spacy_tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "norm_text=[]\n",
    "for i in df['filter_text']:\n",
    "    norm_text.append(tokenize_normalize(str(i)))\n",
    "df['norm_text'] = norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a\n",
    "wordnet_lemm = stem.WordNetLemmatizer()\n",
    "lemword =[]\n",
    "for i in norm_text:\n",
    "    lem_word = []\n",
    "    for j in i:\n",
    "        lem_word.append(wordnet_lemm.lemmatize(j))\n",
    "    lemword.append(lem_word)\n",
    "df['lem_word'] = lemword\n",
    "# turn lem list into string\n",
    "lem_word_list=[]\n",
    "for i in df['lem_word']:\n",
    "    lem_word_list.append(\",\".join(i))\n",
    "df['lem'] = lem_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates\n",
    "ddf = df.drop_duplicates(['lem'])\n",
    "ddf.reset_index(inplace=True,drop=True)\n",
    "dataframe = ddf.drop(['lem_word'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create class for labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text for labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create six classes \n",
    "excitement=['pleased','amused','excited','excitement','exciting','astonished']\n",
    "happy=['glad','delighted','satisfied','happy','joy','love','beabetteryou']\n",
    "pleasant= ['content','at ease','relaxed','serene','pleasant']\n",
    "surprise=[ 'sad','frustration','alarmed','afraid','tense','miserable','positive','RIP']\n",
    "fear=['disgust','depression','fear','terrified','horrible','anxiety','stress','epidemic','worry']\n",
    "angry=['distressed','gloomy','angry','annoyed','offended','discrimination','depressed','anxious','anger','mask']\n",
    "labels =[excitement,happy,pleasant,surprise,fear,angry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input text data for class count\n",
    "countlist = [[],[],[],[],[],[]]\n",
    "for line in ddf.textlist:\n",
    "    for i in range(0,len(countlist)):\n",
    "        count = 0\n",
    "        for tag in line:\n",
    "            if tag in labels[i]:\n",
    "                count += 1\n",
    "        countlist[i].append(count)\n",
    "# write the classlist into dataframe        \n",
    "dataframe['excitement']=countlist[0]\n",
    "dataframe['happy']=countlist[1]\n",
    "dataframe['pleasant']=countlist[2]\n",
    "dataframe['surprise']=countlist[3]\n",
    "dataframe['fear']=countlist[4]\n",
    "dataframe['angry']=countlist[5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## emoji for labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create emoji label\n",
    "excitement_emoji=['😀','🤣','😃','😄','😗','😙','😚','☺️','🙂','🤗','🤩','🏆']\n",
    "happy_emoji=['😆','😉','😊','😋','😎','😍','😘','🥰','🧡', '💛', '💚', '💙', '💜', '🔥']\n",
    "pleasant_emoji= [ '😛','😜','😝','🤭','👏','👍','💘', '💝', '💟']\n",
    "surprise_emoji=[ '😭','😦','😧','😨','😩','🤯','💔','☹️','🙁','😖','😞','😟','🙀']\n",
    "fear_emoji=['😰','😱','🥵','🥶','🆘','😢','😭','🥵','🥶','💩']\n",
    "angry_emoji=['🤢','🤮','🤧','👎','😤','😡','😠','🤬','🤮']\n",
    "labels_emoji =[excitement_emoji,happy_emoji,pleasant_emoji,surprise_emoji,fear_emoji,angry_emoji]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input emoji for class count\n",
    "countlist_emoji = [[],[],[],[],[],[]]\n",
    "for line_emoji in dataframe.emoji:\n",
    "    for i in range(0,len(countlist_emoji)):\n",
    "        count = 0\n",
    "        for emojis in line_emoji:\n",
    "            if emojis in labels_emoji[i]:\n",
    "                count += 1\n",
    "        countlist_emoji[i].append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the classlist_emoji into dataframe \n",
    "dataframe['excitement_emoji']=countlist_emoji[0]\n",
    "dataframe['happy_emoji']=countlist_emoji[1]\n",
    "dataframe['pleasant_emoji']=countlist_emoji[2]\n",
    "dataframe['surprise_emoji']=countlist_emoji[3]\n",
    "dataframe['fear_emoji']=countlist_emoji[4]\n",
    "dataframe['angry_emoji']=countlist_emoji[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the class score both in text and emoji\n",
    "dataframe['excitementclass']=dataframe['excitement']+dataframe['excitement_emoji']\n",
    "dataframe['happyclass']=dataframe['happy']+dataframe['happy_emoji']\n",
    "dataframe['pleasantclass']=dataframe['pleasant']+dataframe['pleasant_emoji']\n",
    "dataframe['surpriseclass']=dataframe['surprise']+dataframe['surprise_emoji']\n",
    "dataframe['fearclass']=dataframe['fear']+dataframe['fear_emoji']\n",
    "dataframe['angryclass']=dataframe['angry']+dataframe['angry_emoji']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# score  and labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the numberic columnm to collect the max index for labeling classes\n",
    "maxdataframe = dataframe[['excitementclass','happyclass','pleasantclass','surpriseclass','fearclass','angryclass']]\n",
    "maxdataframe['max_value'] = maxdataframe.max(axis=1)\n",
    "maxdataframe['max_idx'] = maxdataframe.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maxdataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the max column in dataframe\n",
    "dataframe['max_value'] = maxdataframe['max_value']\n",
    "dataframe['max_idx'] = maxdataframe['max_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "a = dataframe.drop_duplicates(subset=['max_idx'],keep='first')\n",
    "data_st = dataframe.loc[dataframe['max_idx'].isin(a['max_idx'])]\n",
    "data_st = data_st.sort_values('max_idx',ascending=True) \n",
    "#ata_cf = frame.loc[frame['pop'].isin(a['pop'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "excitement = data_st.loc[data_st[\"max_idx\"] == \"excitementclass\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excitement = data_st.loc[data_st[\"max_idx\"] == \"excitementclass\"].head()\n",
    "excitement.to_csv('excitement.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy = data_st.loc[data_st[\"max_idx\"] == \"happyclass\"]\n",
    "happy.to_csv('happy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pleasant = data_st.loc[data_st[\"max_idx\"] == \"pleasantclass\"]\n",
    "pleasant.to_csv('pleasant.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise = data_st.loc[data_st[\"max_idx\"] == \"surpriseclass\"]\n",
    "surprise.to_csv('surprise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fear = data_st.loc[data_st[\"max_idx\"] == \"fearclass\"]\n",
    "fear.to_csv('fear.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "angry = data_st.loc[data_st[\"max_idx\"] == \"angryclass\"]\n",
    "angry.to_csv('angry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
