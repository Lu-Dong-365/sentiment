{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('parser', <spacy.pipeline.pipes.DependencyParser at 0x2126683c4c8>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import preprocessor as p\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk import stem\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nlp = spacy.load('en_core_web_md', disable=['ner'])\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load json into dataframe\n",
    "with open('tweets.json') as f:\n",
    "    data=[]\n",
    "    for line in f:\n",
    "        if line!=\"\\n\":\n",
    "            data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(data)\n",
    "df = d[['id','created_at','text','lang']]\n",
    "#df['hashtags']= np.array([i['entities']['hashtags']for i in data])\n",
    "#df['hashtags']= [i['entities']['hashtags']for i in data]\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.238599e+18</td>\n",
       "      <td>Fri Mar 13 22:51:34 +0000 2020</td>\n",
       "      <td>@FinisimaPersona üëç https://t.co/Erego0NAgb</td>\n",
       "      <td>und</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.238599e+18</td>\n",
       "      <td>Fri Mar 13 22:51:34 +0000 2020</td>\n",
       "      <td>RT @randallbell: 'better take my shirt off for...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.238599e+18</td>\n",
       "      <td>Fri Mar 13 22:51:34 +0000 2020</td>\n",
       "      <td>RT @rosapantin1301: De China sale un virus que...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.238599e+18</td>\n",
       "      <td>Fri Mar 13 22:51:34 +0000 2020</td>\n",
       "      <td>RT @LasVegasLocally: A @WetRepublic employee h...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.238599e+18</td>\n",
       "      <td>Fri Mar 13 22:51:34 +0000 2020</td>\n",
       "      <td>RT @maxvayshia: Nothing survives here. Not dre...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112795</td>\n",
       "      <td>1.238691e+18</td>\n",
       "      <td>Sat Mar 14 04:57:44 +0000 2020</td>\n",
       "      <td>RT @ilovehye: Ïú†ÎüΩÏóê ÏôÄÎ≥¥ÏÑ∏Ïöî. ÎßàÏä§ÌÅ¨ Ïì¥ ÏÇ¨Îûå ÌïúÎ™ÖÎèÑ ÏóÜÏäµÎãàÎã§. ÏôúÎÉêÍµ¨...</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112796</td>\n",
       "      <td>1.238691e+18</td>\n",
       "      <td>Sat Mar 14 04:57:45 +0000 2020</td>\n",
       "      <td>RT @thestandardth: ‡∏ú‡∏•‡πÇ‡∏û‡∏•‡πÄ‡∏ú‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤...</td>\n",
       "      <td>th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112797</td>\n",
       "      <td>1.238691e+18</td>\n",
       "      <td>Sat Mar 14 04:57:45 +0000 2020</td>\n",
       "      <td>RT @ggreenwald: One day we‚Äôll find out how man...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112798</td>\n",
       "      <td>1.238691e+18</td>\n",
       "      <td>Sat Mar 14 04:57:45 +0000 2020</td>\n",
       "      <td>RT @tristanmom2: Regular surgical masks does n...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112799</td>\n",
       "      <td>1.238691e+18</td>\n",
       "      <td>Sat Mar 14 04:57:47 +0000 2020</td>\n",
       "      <td>RT @DasshaRumyanaya: My pussy loves cum, fill ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112800 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                      created_at  \\\n",
       "0       1.238599e+18  Fri Mar 13 22:51:34 +0000 2020   \n",
       "1       1.238599e+18  Fri Mar 13 22:51:34 +0000 2020   \n",
       "2       1.238599e+18  Fri Mar 13 22:51:34 +0000 2020   \n",
       "3       1.238599e+18  Fri Mar 13 22:51:34 +0000 2020   \n",
       "4       1.238599e+18  Fri Mar 13 22:51:34 +0000 2020   \n",
       "...              ...                             ...   \n",
       "112795  1.238691e+18  Sat Mar 14 04:57:44 +0000 2020   \n",
       "112796  1.238691e+18  Sat Mar 14 04:57:45 +0000 2020   \n",
       "112797  1.238691e+18  Sat Mar 14 04:57:45 +0000 2020   \n",
       "112798  1.238691e+18  Sat Mar 14 04:57:45 +0000 2020   \n",
       "112799  1.238691e+18  Sat Mar 14 04:57:47 +0000 2020   \n",
       "\n",
       "                                                     text lang  \n",
       "0              @FinisimaPersona üëç https://t.co/Erego0NAgb  und  \n",
       "1       RT @randallbell: 'better take my shirt off for...   en  \n",
       "2       RT @rosapantin1301: De China sale un virus que...   es  \n",
       "3       RT @LasVegasLocally: A @WetRepublic employee h...   en  \n",
       "4       RT @maxvayshia: Nothing survives here. Not dre...   en  \n",
       "...                                                   ...  ...  \n",
       "112795  RT @ilovehye: Ïú†ÎüΩÏóê ÏôÄÎ≥¥ÏÑ∏Ïöî. ÎßàÏä§ÌÅ¨ Ïì¥ ÏÇ¨Îûå ÌïúÎ™ÖÎèÑ ÏóÜÏäµÎãàÎã§. ÏôúÎÉêÍµ¨...   ko  \n",
       "112796  RT @thestandardth: ‡∏ú‡∏•‡πÇ‡∏û‡∏•‡πÄ‡∏ú‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤...   th  \n",
       "112797  RT @ggreenwald: One day we‚Äôll find out how man...   en  \n",
       "112798  RT @tristanmom2: Regular surgical masks does n...   en  \n",
       "112799  RT @DasshaRumyanaya: My pussy loves cum, fill ...   en  \n",
       "\n",
       "[112800 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.238599e+18</td>\n",
       "      <td>Fri Mar 13 22:51:34 +0000 2020</td>\n",
       "      <td>@FinisimaPersona üëç https://t.co/Erego0NAgb</td>\n",
       "      <td>und</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.238599e+18</td>\n",
       "      <td>Fri Mar 13 22:51:34 +0000 2020</td>\n",
       "      <td>RT @randallbell: 'better take my shirt off for...</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.238599e+18</td>\n",
       "      <td>Fri Mar 13 22:51:34 +0000 2020</td>\n",
       "      <td>RT @rosapantin1301: De China sale un virus que...</td>\n",
       "      <td>es</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.238599e+18</td>\n",
       "      <td>Fri Mar 13 22:51:34 +0000 2020</td>\n",
       "      <td>RT @LasVegasLocally: A @WetRepublic employee h...</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.238599e+18</td>\n",
       "      <td>Fri Mar 13 22:51:34 +0000 2020</td>\n",
       "      <td>RT @maxvayshia: Nothing survives here. Not dre...</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112795</td>\n",
       "      <td>1.238691e+18</td>\n",
       "      <td>Sat Mar 14 04:57:44 +0000 2020</td>\n",
       "      <td>RT @ilovehye: Ïú†ÎüΩÏóê ÏôÄÎ≥¥ÏÑ∏Ïöî. ÎßàÏä§ÌÅ¨ Ïì¥ ÏÇ¨Îûå ÌïúÎ™ÖÎèÑ ÏóÜÏäµÎãàÎã§. ÏôúÎÉêÍµ¨...</td>\n",
       "      <td>ko</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112796</td>\n",
       "      <td>1.238691e+18</td>\n",
       "      <td>Sat Mar 14 04:57:45 +0000 2020</td>\n",
       "      <td>RT @thestandardth: ‡∏ú‡∏•‡πÇ‡∏û‡∏•‡πÄ‡∏ú‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤...</td>\n",
       "      <td>th</td>\n",
       "      <td>[{'text': '‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏≤‡∏Å‡∏≠‡∏ô‡∏≤‡∏°‡∏±‡∏¢', 'indices': [53, 67]}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112797</td>\n",
       "      <td>1.238691e+18</td>\n",
       "      <td>Sat Mar 14 04:57:45 +0000 2020</td>\n",
       "      <td>RT @ggreenwald: One day we‚Äôll find out how man...</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112798</td>\n",
       "      <td>1.238691e+18</td>\n",
       "      <td>Sat Mar 14 04:57:45 +0000 2020</td>\n",
       "      <td>RT @tristanmom2: Regular surgical masks does n...</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112799</td>\n",
       "      <td>1.238691e+18</td>\n",
       "      <td>Sat Mar 14 04:57:47 +0000 2020</td>\n",
       "      <td>RT @DasshaRumyanaya: My pussy loves cum, fill ...</td>\n",
       "      <td>en</td>\n",
       "      <td>[{'text': 'onlyfans', 'indices': [76, 85]}, {'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112800 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                      created_at  \\\n",
       "0       1.238599e+18  Fri Mar 13 22:51:34 +0000 2020   \n",
       "1       1.238599e+18  Fri Mar 13 22:51:34 +0000 2020   \n",
       "2       1.238599e+18  Fri Mar 13 22:51:34 +0000 2020   \n",
       "3       1.238599e+18  Fri Mar 13 22:51:34 +0000 2020   \n",
       "4       1.238599e+18  Fri Mar 13 22:51:34 +0000 2020   \n",
       "...              ...                             ...   \n",
       "112795  1.238691e+18  Sat Mar 14 04:57:44 +0000 2020   \n",
       "112796  1.238691e+18  Sat Mar 14 04:57:45 +0000 2020   \n",
       "112797  1.238691e+18  Sat Mar 14 04:57:45 +0000 2020   \n",
       "112798  1.238691e+18  Sat Mar 14 04:57:45 +0000 2020   \n",
       "112799  1.238691e+18  Sat Mar 14 04:57:47 +0000 2020   \n",
       "\n",
       "                                                     text lang  \\\n",
       "0              @FinisimaPersona üëç https://t.co/Erego0NAgb  und   \n",
       "1       RT @randallbell: 'better take my shirt off for...   en   \n",
       "2       RT @rosapantin1301: De China sale un virus que...   es   \n",
       "3       RT @LasVegasLocally: A @WetRepublic employee h...   en   \n",
       "4       RT @maxvayshia: Nothing survives here. Not dre...   en   \n",
       "...                                                   ...  ...   \n",
       "112795  RT @ilovehye: Ïú†ÎüΩÏóê ÏôÄÎ≥¥ÏÑ∏Ïöî. ÎßàÏä§ÌÅ¨ Ïì¥ ÏÇ¨Îûå ÌïúÎ™ÖÎèÑ ÏóÜÏäµÎãàÎã§. ÏôúÎÉêÍµ¨...   ko   \n",
       "112796  RT @thestandardth: ‡∏ú‡∏•‡πÇ‡∏û‡∏•‡πÄ‡∏ú‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤...   th   \n",
       "112797  RT @ggreenwald: One day we‚Äôll find out how man...   en   \n",
       "112798  RT @tristanmom2: Regular surgical masks does n...   en   \n",
       "112799  RT @DasshaRumyanaya: My pussy loves cum, fill ...   en   \n",
       "\n",
       "                                                 hashtags  \n",
       "0                                                      []  \n",
       "1                                                      []  \n",
       "2                                                      []  \n",
       "3                                                      []  \n",
       "4                                                      []  \n",
       "...                                                   ...  \n",
       "112795                                                 []  \n",
       "112796   [{'text': '‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏≤‡∏Å‡∏≠‡∏ô‡∏≤‡∏°‡∏±‡∏¢', 'indices': [53, 67]}]  \n",
       "112797                                                 []  \n",
       "112798                                                 []  \n",
       "112799  [{'text': 'onlyfans', 'indices': [76, 85]}, {'...  \n",
       "\n",
       "[112800 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in case entities is not existing\n",
    "hashtagspre=[]\n",
    "for i in data:\n",
    "    if 'entities'in i.keys():\n",
    "        hashtagspre.append(i['entities']['hashtags'])\n",
    "    else:\n",
    "        hashtagspre.append('')\n",
    "df['hashtags'] = hashtagspre\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text in hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract hashtags\n",
    "hashtags_list = list(df.hashtags)\n",
    "all_list=[]\n",
    "for item in hashtags_list:\n",
    "    text_list = []\n",
    "    for i in item:\n",
    "        text_list.append(i['text'].lower())\n",
    "    all_list.append(text_list)\n",
    "df['textlist'] = all_list\n",
    "\n",
    "## extract english text \n",
    "df = df[df['lang']=='en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select emojis\n",
    "def extract_emojis(str):\n",
    "    return ''.join(c for c in str if c in emoji.UNICODE_EMOJI)\n",
    "emoji_list=[]\n",
    "for i in df['text']:\n",
    "    emoji_list.append(extract_emojis(str(i)))\n",
    "df['emoji'] = emoji_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor clean data\n",
    "clean_text=[]\n",
    "for i in df['text']:\n",
    "    clean_text.append(p.clean(str(i)))\n",
    "df['clean_text'] = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data function\n",
    "def clean_tweets(tweet):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    #after tweepy preprocessing the colon symbol left remain after      #removing mentions\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'‚Äö√Ñ¬∂', '', tweet)\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "#    #filter using NLTK library append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_tweet = []\n",
    "    #looping through conditions\n",
    "    for w in word_tokens:\n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in string.punctuation:\n",
    "            filtered_tweet.append(w)\n",
    "#    return filtered_tweet\n",
    "    return ' '.join(filtered_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling clean data \n",
    "filter_text=[]\n",
    "for i in df['clean_text']:\n",
    "    filter_text.append(clean_tweets(str(i)))\n",
    "df['filter_text'] = filter_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob\n",
    "#from textblob import TextBlob\n",
    "#blob = TextBlob(filtered_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Tokenize\n",
    "def spacy_tokenize(string):\n",
    "    tokens = list()\n",
    "    doc = nlp(string)\n",
    "    for token in doc:\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Normalize\n",
    "def normalize(tokens):\n",
    "    normalized_tokens = list()\n",
    "    for token in tokens:\n",
    "        normalized = token.text.lower().strip()\n",
    "        if ((token.is_alpha or token.is_digit)):\n",
    "            normalized_tokens.append(normalized)\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Tokenize and normalize\n",
    "def tokenize_normalize(string):\n",
    "    return normalize(spacy_tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "norm_text=[]\n",
    "for i in df['filter_text']:\n",
    "    norm_text.append(tokenize_normalize(str(i)))\n",
    "df['norm_text'] = norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a\n",
    "wordnet_lemm = stem.WordNetLemmatizer()\n",
    "lemword =[]\n",
    "for i in norm_text:\n",
    "    lem_word = []\n",
    "    for j in i:\n",
    "        lem_word.append(wordnet_lemm.lemmatize(j))\n",
    "    lemword.append(lem_word)\n",
    "df['lem_word'] = lemword\n",
    "# turn lem list into string\n",
    "lem_word_list=[]\n",
    "for i in df['lem_word']:\n",
    "    lem_word_list.append(\",\".join(i))\n",
    "df['lem'] = lem_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates\n",
    "ddf = df.drop_duplicates(['lem'])\n",
    "ddf.reset_index(inplace=True,drop=True)\n",
    "dataframe = ddf.drop(['lem_word'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create class for labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text for labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create six classes \n",
    "excitement=['pleased','amused','excited','excitement','exciting','astonished']\n",
    "happy=['glad','delighted','satisfied','happy','joy','love','beabetteryou']\n",
    "pleasant= ['content','at ease','relaxed','serene','pleasant']\n",
    "surprise=[ 'sad','frustration','alarmed','afraid','tense','miserable','positive','RIP']\n",
    "fear=['disgust','depression','fear','terrified','horrible','anxiety','stress','epidemic','worry']\n",
    "angry=['distressed','gloomy','angry','annoyed','offended','discrimination','depressed','anxious','anger','mask']\n",
    "labels =[excitement,happy,pleasant,surprise,fear,angry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input text data for class count\n",
    "countlist = [[],[],[],[],[],[]]\n",
    "for line in ddf.textlist:\n",
    "    for i in range(0,len(countlist)):\n",
    "        count = 0\n",
    "        for tag in line:\n",
    "            if tag in labels[i]:\n",
    "                count += 1\n",
    "        countlist[i].append(count)\n",
    "# write the classlist into dataframe        \n",
    "dataframe['excitement']=countlist[0]\n",
    "dataframe['happy']=countlist[1]\n",
    "dataframe['pleasant']=countlist[2]\n",
    "dataframe['surprise']=countlist[3]\n",
    "dataframe['fear']=countlist[4]\n",
    "dataframe['angry']=countlist[5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## emoji for labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create emoji label\n",
    "excitement_emoji=['üòÄ','ü§£','üòÉ','üòÑ','üòó','üòô','üòö','‚ò∫Ô∏è','üôÇ','ü§ó','ü§©','üèÜ']\n",
    "happy_emoji=['üòÜ','üòâ','üòä','üòã','üòé','üòç','üòò','ü•∞','üß°', 'üíõ', 'üíö', 'üíô', 'üíú', 'üî•']\n",
    "pleasant_emoji= [ 'üòõ','üòú','üòù','ü§≠','üëè','üëç','üíò', 'üíù', 'üíü']\n",
    "surprise_emoji=[ 'üò≠','üò¶','üòß','üò®','üò©','ü§Ø','üíî','‚òπÔ∏è','üôÅ','üòñ','üòû','üòü','üôÄ']\n",
    "fear_emoji=['üò∞','üò±','ü•µ','ü•∂','üÜò','üò¢','üò≠','ü•µ','ü•∂','üí©']\n",
    "angry_emoji=['ü§¢','ü§Æ','ü§ß','üëé','üò§','üò°','üò†','ü§¨','ü§Æ']\n",
    "labels_emoji =[excitement_emoji,happy_emoji,pleasant_emoji,surprise_emoji,fear_emoji,angry_emoji]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input emoji for class count\n",
    "countlist_emoji = [[],[],[],[],[],[]]\n",
    "for line_emoji in dataframe.emoji:\n",
    "    for i in range(0,len(countlist_emoji)):\n",
    "        count = 0\n",
    "        for emojis in line_emoji:\n",
    "            if emojis in labels_emoji[i]:\n",
    "                count += 1\n",
    "        countlist_emoji[i].append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the classlist_emoji into dataframe \n",
    "dataframe['excitement_emoji']=countlist_emoji[0]\n",
    "dataframe['happy_emoji']=countlist_emoji[1]\n",
    "dataframe['pleasant_emoji']=countlist_emoji[2]\n",
    "dataframe['surprise_emoji']=countlist_emoji[3]\n",
    "dataframe['fear_emoji']=countlist_emoji[4]\n",
    "dataframe['angry_emoji']=countlist_emoji[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the class score both in text and emoji\n",
    "dataframe['excitementclass']=dataframe['excitement']+dataframe['excitement_emoji']\n",
    "dataframe['happyclass']=dataframe['happy']+dataframe['happy_emoji']\n",
    "dataframe['pleasantclass']=dataframe['pleasant']+dataframe['pleasant_emoji']\n",
    "dataframe['surpriseclass']=dataframe['surprise']+dataframe['surprise_emoji']\n",
    "dataframe['fearclass']=dataframe['fear']+dataframe['fear_emoji']\n",
    "dataframe['angryclass']=dataframe['angry']+dataframe['angry_emoji']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# score  and labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the numberic columnm to collect the max index for labeling classes\n",
    "maxdataframe = dataframe[['excitementclass','happyclass','pleasantclass','surpriseclass','fearclass','angryclass']]\n",
    "maxdataframe['max_value'] = maxdataframe.max(axis=1)\n",
    "maxdataframe['max_idx'] = maxdataframe.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maxdataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the max column in dataframe\n",
    "dataframe['max_value'] = maxdataframe['max_value']\n",
    "dataframe['max_idx'] = maxdataframe['max_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "a = dataframe.drop_duplicates(subset=['max_idx'],keep='first')\n",
    "data_st = dataframe.loc[dataframe['max_idx'].isin(a['max_idx'])]\n",
    "data_st = data_st.sort_values('max_idx',ascending=True) \n",
    "#ata_cf = frame.loc[frame['pop'].isin(a['pop'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "excitement = data_st.loc[data_st[\"max_idx\"] == \"excitementclass\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excitement = data_st.loc[data_st[\"max_idx\"] == \"excitementclass\"].head()\n",
    "excitement.to_csv('excitement.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy = data_st.loc[data_st[\"max_idx\"] == \"happyclass\"]\n",
    "happy.to_csv('happy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pleasant = data_st.loc[data_st[\"max_idx\"] == \"pleasantclass\"]\n",
    "pleasant.to_csv('pleasant.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise = data_st.loc[data_st[\"max_idx\"] == \"surpriseclass\"]\n",
    "surprise.to_csv('surprise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fear = data_st.loc[data_st[\"max_idx\"] == \"fearclass\"]\n",
    "fear.to_csv('fear.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "angry = data_st.loc[data_st[\"max_idx\"] == \"angryclass\"]\n",
    "angry.to_csv('angry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
